{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corsica Example\n",
    "\n",
    "The [eqasim](https://github.com/eqasim-org/eqasim-java) repository is a packaged version of MATSim which makes use of discrete choice models instead of the scoring process for decision making. It uses a small simulation scenario for Corsica for unit testing. This scenario is packaged with the eqasim repository, so it can be used here easily for testing.\n",
    "\n",
    "The following will show how to set up the use case (in this notebook) and then perform a basic calibration task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Note that the calibration steps will take a while since the scenario, although it is small, still need to be run for 40 iterations and multiple times.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case\n",
    "\n",
    "First, we clone the `eqasim` repository into `eqasim-java`. For that, `git` needs to be installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only if you have git available\n",
    "!git clone https://github.com/eqasim-org/eqasim-java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you don't have git installed on the command line, you can use other tools to clone the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to build the executable jar. For that, Maven needs to be installed on the command line. If you don't have it available, you can use an IDE like IntelliJ or Eclipse to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd eqasim-java && mvn -Pstandalone package --projects ile_de_france --also-make -DskipTests=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will find the latest executable jar generated by Maven:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re\n",
    "\n",
    "potential_jars = glob.glob(\"eqasim-java/ile_de_france/target/*.jar\")\n",
    "executable_jar = [\n",
    "    jar for jar in potential_jars\n",
    "    if re.search(r\"/ile_de_france-[0-9.]+\\.jar\", jar)\n",
    "]\n",
    "\n",
    "if len(executable_jar) != 1:\n",
    "    raise RuntimeError(\"Could not find executable jar\")\n",
    "\n",
    "executable_jar = executable_jar[0]\n",
    "executable_jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulator\n",
    "\n",
    "To run the calibration, we need to define the simulator. For MATSim, we already have a prepackaged version, which we create like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "from octras.matsim import MATSimSimulator\n",
    "\n",
    "if os.path.exists(\"eqasim_example_working_directory\"):\n",
    "    shutil.rmtree(\"eqasim_example_working_directory\")\n",
    "\n",
    "os.mkdir(\"eqasim_example_working_directory\")\n",
    "\n",
    "simulator = MATSimSimulator(\n",
    "    working_directory = \"eqasim_example_working_directory\", # Working directory, which must exist.\n",
    "    parameters = dict(\n",
    "        class_path = executable_jar,\n",
    "        main_class = \"org.eqasim.ile_de_france.RunSimulation\",\n",
    "        iterations = 40,\n",
    "        arguments = [\n",
    "            \"--config-path\", \n",
    "            \"eqasim-java/ile_de_france/src/main/resources/corsica/corsica_config.xml\"\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see above, we create a working directory where the simulator will save all temporary data. Furthermore, we define some default arguments to the simulator (here based on eqasim).\n",
    "\n",
    "We can test by running two iterations of this simulation. This takes a while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging which we would normally see on the command line\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(\"octras\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "import time\n",
    "\n",
    "# We define a run and override the \"iterations\" parameter:\n",
    "print(\"Starting run ...\")\n",
    "simulator.run(\"my_test_run\", parameters = dict(\n",
    "    iterations = 2\n",
    "))\n",
    "\n",
    "# The simulation is now running in the background. We can wait for completion like so:\n",
    "print(\"Still running ...\")\n",
    "while not simulator.ready(\"my_test_run\"):\n",
    "    time.sleep(1.0)\n",
    "    \n",
    "# Afterwards, as we have no need for them for the time being, we can clean the temporarily saved output data:\n",
    "simulator.clean(\"my_test_run\")\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now set up the calibration problem as class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from octras import Problem\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class CorsicaProblem(Problem):\n",
    "    def __init__(self, reference_share):\n",
    "        # Our goal is to calibrate the mode share of the \"car\" mode in the simulation while \n",
    "        # modifying the alternative-specific constant (ASC) of the mode. We make the reference\n",
    "        # share configurable form outside the problem.\n",
    "        self.reference_share = reference_share\n",
    "        \n",
    "    def get_information(self):\n",
    "        # Here, we tell the optimizer that the problem has one parameter (the ASC)\n",
    "        # and we provide the initial value for calibration, which can be used by\n",
    "        # the optimizer (depending on which we choose). As we use only one parameters\n",
    "        # (the ASC), we provide only one value.\n",
    "        \n",
    "        return dict(\n",
    "            number_of_parameters = 1,\n",
    "            initial_values = [0.0]\n",
    "        )\n",
    "\n",
    "    def parameterize(self, x):\n",
    "        # This method translates a numeric list (x) into instructions for the simulator. In\n",
    "        # this case, we use the generic MATSim simulator implementation. To append arguments to\n",
    "        # the command line call of MATSim, we use the \"arguments\" return value. It will NOT override\n",
    "        # the value from above, but instead also append these commands to the existing command.\n",
    "        return dict(\n",
    "            arguments = [\"--mode-choice-parameter:car.alpha_u\", x[0]]\n",
    "        )\n",
    "\n",
    "    def evaluate(self, x, path):\n",
    "        # Here, we are should return an objective value. To guide the optimization, we calculate a \n",
    "        # deviation of the currently achieved mode share after the simulation with the reference\n",
    "        # value. To do so, MATSim provides us the output path of the MATSim simulation where we can\n",
    "        # access all the relevant results.\n",
    "        \n",
    "        # Read the modestats file as CSV\n",
    "        mode_stats_paths = glob.glob(\"%s/modestats.txt\" % path)\n",
    "        df = pd.read_csv(mode_stats_paths[0], sep = \"\\t\")\n",
    "\n",
    "        # Get share of car trips in the last iteration\n",
    "        simulation_share = df[\"car\"].values[-1]\n",
    "\n",
    "        # Construct the obejctive value\n",
    "        objective = np.abs(simulation_share - self.reference_share)\n",
    "\n",
    "        # Return objective\n",
    "        return objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization loop\n",
    "\n",
    "Now, we can set up the optimization loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from octras import Evaluator\n",
    "from octras.algorithms import CMAES\n",
    "\n",
    "# We already have the simulator\n",
    "simulator\n",
    "\n",
    "# We create an instance of the problem\n",
    "problem = CorsicaProblem(reference_share = 0.3) # And we target 30% share\n",
    "\n",
    "# We need to create an \"evaluator\" which is a generic bridge between the optimizers and the simulators\n",
    "evaluator = Evaluator(\n",
    "    problem = problem,\n",
    "    simulator = simulator,\n",
    "    parallel = 1 # Here we could say that we allow N simulations in parallel\n",
    ")\n",
    "\n",
    "# We need to set up the optimization algorithm, here CMA-ES\n",
    "algorithm = CMAES(\n",
    "    problem = problem,\n",
    "    initial_step_size = 0.1,\n",
    "    seed = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can run one optimization step with the CMA-ES algorithm. Note that this means that multiple evaluations will be performed on the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now perform one step of the algorithm. This may mean that it calls the evaluator multiple times with \n",
    "# different values, for instance for the estimation of gradients or similar tasks.\n",
    "algorithm.advance(evaluator)\n",
    "\n",
    "# Afterwards, we can obtain the trace of all the specific calls by calling fetch_trace on the evaluator.\n",
    "# Those traces contain a lot of information provided by the problem, the simulator and the evaluator and \n",
    "# can be used for detailed and customized analysis.\n",
    "trace = evaluator.fetch_trace()\n",
    "\n",
    "# As a shortcut, we can also, for instance, just obtain the current number of evaluations\n",
    "evaluator.current_evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call the above block multiple times to advance the algorithm.\n",
    "\n",
    "However, it is more convenient to automatically run in a loop. For that, we can set up a tracker which is notified whenever there is new results after evaluating the simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# For plotting\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class MyTracker:\n",
    "    def __init__(self):\n",
    "        # We track the best and current objective for plotting\n",
    "        self.best_objectives = []\n",
    "        self.current_objectives = []\n",
    "        \n",
    "        # Also, we track the proposed parameter values that correspond to them\n",
    "        self.best_parameters = []\n",
    "        self.current_parameters = []\n",
    "        \n",
    "    def notify(self, evaluation):\n",
    "        # We are notified by each evaluation\n",
    "\n",
    "        best_value = np.inf if len(self.best_objectives) == 0 else self.best_objectives[-1]\n",
    "        best_parameter_value = np.nan if len(self.best_objectives) == 0 else self.best_parameters[-1]\n",
    "\n",
    "        if evaluation[\"objective\"] < best_value:\n",
    "            best_value = evaluation[\"objective\"]\n",
    "            best_parameter_value = evaluation[\"x\"][0]\n",
    "\n",
    "        self.best_objectives.append(best_value)\n",
    "        self.current_objectives.append(evaluation[\"objective\"])\n",
    "        \n",
    "        self.best_parameters.append(best_parameter_value)\n",
    "        self.current_parameters.append(evaluation[\"x\"][0])\n",
    "        \n",
    "        self.plot()\n",
    "        \n",
    "    def plot(self):\n",
    "        clear_output(wait = True)\n",
    "\n",
    "        plt.figure(figsize = (10, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.current_objectives, 'ok')\n",
    "        plt.plot(self.best_objectives, 'r')\n",
    "        plt.xlabel(\"Evaluation\")\n",
    "        plt.ylabel(\"Objective\")\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.current_parameters, 'ok')\n",
    "        plt.plot(self.best_parameters, 'r')\n",
    "        plt.xlabel(\"Evaluation\")\n",
    "        plt.ylabel(\"Mode-specific constant\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "tracker = MyTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start the loop and we let it run until 50 iterations have been obtained. If you want, you can restart the cell to add even more iterations. You should see how the objective value is increasing getting close to zero, which means that the mode share has been pushed towards 30% by calibrating the mode specific constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from octras import Loop\n",
    "\n",
    "Loop(\n",
    "   maximum_evaluations = 50 # We run 50x 40 iterations \n",
    ").run(\n",
    "    evaluator = evaluator,\n",
    "    algorithm = algorithm,\n",
    "    tracker = tracker\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should look somewhat like this:\n",
    "<img src=\"result_corsica.png\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
